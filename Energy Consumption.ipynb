{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "version": "2.7.11", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python", 
            "name": "python2-spark20"
        }
    }, 
    "cells": [
        {
            "outputs": [
                {
                    "metadata": {}, 
                    "execution_count": 21, 
                    "data": {
                        "text/plain": "[Row(UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity=u'117'),\n Row(UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity=u'16'),\n Row(UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity=u'15'),\n Row(UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity=u'25'),\n Row(UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity=u'32')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 21, 
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "outputs": [
                {
                    "metadata": {}, 
                    "execution_count": 22, 
                    "data": {
                        "text/plain": "[Row(domestic_gas ;heating_gas=u'0.096226455 ;0.366193236'),\n Row(domestic_gas ;heating_gas=u'0.322599638 ;0.57959223'),\n Row(domestic_gas ;heating_gas=u'0.032705972 ;0.036460695'),\n Row(domestic_gas ;heating_gas=u'0.02750427 ;0.23466382'),\n Row(domestic_gas ;heating_gas=u'0.322599638 ;0.57959223')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 22, 
            "metadata": {}, 
            "source": "dfHDD = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('Exploring', 'HDD-Features.csv'))\ndfHDD.take(5)"
        }, 
        {
            "outputs": [
                {
                    "metadata": {}, 
                    "execution_count": 23, 
                    "data": {
                        "text/plain": "[Row(Property Name ;plug_load_consumption ;ac_consumption ;domestic_gas ;heating_gas=u'ChurchofStCeciliaReport ;1.165.140.596;0.983531348 ;0.096226455 ;0.366193236'),\n Row(Property Name ;plug_load_consumption ;ac_consumption ;domestic_gas ;heating_gas=u'69thLaneStudio ;3.276.988.438;5.008.371.873;0.322599638 ;0.57959223'),\n Row(Property Name ;plug_load_consumption ;ac_consumption ;domestic_gas ;heating_gas=u'UnitarianChurchofStatenIsland ;2.345.049.272;0.296133819 ;0.032705972 ;0.036460695'),\n Row(Property Name ;plug_load_consumption ;ac_consumption ;domestic_gas ;heating_gas=u'SSolowayandSonsPIPPrinting ;4.618.817.159;0.765188561 ;0.02750427 ;0.23466382'),\n Row(Property Name ;plug_load_consumption ;ac_consumption ;domestic_gas ;heating_gas=u'SunnysideJewishCenterReport ;9.323.896.186;123.432.624;0.322599638 ;0.57959223')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 23, 
            "metadata": {}, 
            "source": "dfCH = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(bmos.url('Exploring', 'CDD-HDD-Features.csv'))\ndfCH.take(5)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 24, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import numpy as np\nimport pandas as pd\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "df2.show(5)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 26, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# define cleaning functions\ndef energy(v): # reformat the values to get an actual number (e.g., 117,870 kWh to 117870)\n    if (v=='') or v==None: return np.nan\n    v = v.encode('ascii','ignore').split(' ')[0].replace(',','')\n    return np.nan if(v=='') else float(v)\ndef age(v): # computes the age of a buildings, given the year of construction\n    if (v=='') or v==None: return np.nan\n    v = v.encode('ascii','ignore')\n    return 2016.0-float(v) if(len(v)==4) else np.nan\ndef stories(v):\n    if (v=='') or v==None: return np.nan\n    return float(v)\ndef sqFeet(v): # get rid of commas \n    if (v=='') or v==None: return np.nan\n    v = v.encode('ascii','ignore').replace(',','')\n    return np.nan if(v=='') else float(v) \ndef plei(v): # in the PLEI columns, missing values can be interpeted as 0 plugged equipment\n    try:\n        vv = float(v)\n    except:\n        vv = 0.0\n    return vv \n# Define udf's to apply the defined function to the Spark DataFrame\nudfEnergy = udf(energy, DoubleType())\nudfAge = udf(age, DoubleType())\nudfStories = udf(stories, DoubleType())\nudfSqFeet = udf(sqFeet, DoubleType())\nudfPlei = udf(plei, DoubleType())"
        }, 
        {
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-27-f7ae7d3da724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UTSUM_Electricity_Usage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfEnergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UTSUM_Electricity_Usage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Year of Construction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfAge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Year of Construction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Number of Stories\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfStories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Number of Stories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Total Square Feet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfSqFeet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Total Square Feet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_1_Quantity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfPlei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_1_Quantity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_3_Quantity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudfPlei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_3_Quantity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UTSUM_Electricity_Usage\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"energy\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Year of Construction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Number of Stories\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"number_stories\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"INFO_Total Square Feet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"square_feet\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_1_Quantity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"plei_1\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEI_3_Quantity\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"plei_3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark20master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \"\"\"\n\u001b[1;32m   1369\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark20master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`UTSUM_Electricity_Usage`' given input columns: [UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity];;\\n'Project [UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity#41, energy('UTSUM_Electricity_Usage) AS UTSUM_Electricity_Usage#60]\\n+- Relation[UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity#41] csv\\n\""
                    ], 
                    "evalue": "u\"cannot resolve '`UTSUM_Electricity_Usage`' given input columns: [UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity];;\\n'Project [UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity#41, energy('UTSUM_Electricity_Usage) AS UTSUM_Electricity_Usage#60]\\n+- Relation[UTSUM_Electricity_Usage ;INFO_Year of Construction ;INFO_Number of Stories ;INFO_Total Square Feet ;PLEI_1_Quantity ;PLEI_3_Quantity#41] csv\\n\"", 
                    "ename": "AnalysisException"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 27, 
            "metadata": {}, 
            "source": "dfN = df2.withColumn(\"UTSUM_Electricity_Usage\", udfEnergy(\"UTSUM_Electricity_Usage\")) \\\n         .withColumn(\"INFO_Year of Construction\", udfAge(\"INFO_Year of Construction\")) \\\n         .withColumn(\"INFO_Number of Stories\", udfStories(\"INFO_Number of Stories\")) \\\n         .withColumn(\"INFO_Total Square Feet\", udfSqFeet(\"INFO_Total Square Feet\")) \\\n         .withColumn(\"PLEI_1_Quantity\", udfPlei(\"PLEI_1_Quantity\")) \\\n         .withColumn(\"PLEI_3_Quantity\", udfPlei(\"PLEI_3_Quantity\")).cache()\ndfN = dfN.withColumnRenamed(\"UTSUM_Electricity_Usage\",\"energy\") \\\n           .withColumnRenamed(\"INFO_Year of Construction\",\"age\") \\\n           .withColumnRenamed(\"INFO_Number of Stories\",\"number_stories\") \\\n           .withColumnRenamed(\"INFO_Total Square Feet\",\"square_feet\") \\\n           .withColumnRenamed(\"PLEI_1_Quantity\",\"plei_1\") \\\n           .withColumnRenamed(\"PLEI_3_Quantity\",\"plei_3\") "
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "dfN.take(1)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# compute average of non-missing energy and age\nenergy_mean = np.nanmean(np.asarray(dfN.select(\"energy\").rdd.map(lambda r: r[0]).collect()))\nage_mean = np.nanmean(np.asarray(dfN.select(\"age\").rdd.map(lambda r: r[0]).collect()))\n# fill missing values with the computed average\ndfN = dfN.na.fill({\"energy\": energy_mean, \"age\": age_mean})"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# define Spark DataFrame to be written to our object store\ndfOut = dfN.select('energy', 'age', 'number_stories','square_feet','plei_1','plei_3')"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# use the .toPandas() function to map Spark DataFrames to pandas DataFrames\ndfNp = dfN.toPandas()\ndfHDDp = dfHDD.toPandas()\n# concatenate two pandas DataFrames\nfeat = pd.concat([dfNp, dfHDDp], axis=1)\n# get the column names of the concatenated DataFrame\ncols = feat.columns\n# scale data to prepare for regression model \nfrom sklearn import preprocessing\nscaler = preprocessing.MaxAbsScaler() \nfeat = scaler.fit_transform(feat)\n# define a new DataFrame with the scaled data\ndfScaled = pd.DataFrame(feat,columns=cols)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('bmh')\nff = pd.plotting.scatter_matrix(dfScaled, diagonal='hist',figsize=(12,12))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Fit a linear regression model to the observed data\n\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data. The energy usage (kWh) in buildings can be explained by considering the following building characteristics:\n\n    Age of the building\n    Square feet\n    Number of stories\n    Total number of plugged equipment\n\nThe regression modeler takes those building characteristics, which you prepared and scaled in an earlier step, and calculates the association between the proposed predicted value and the observed energy usage of the building."
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# get a list of the features used to explain energy\nfeatures = dfScaled.columns.tolist()\nresponse = ['energy']\nfeatures.remove(response[0])\n# import regression solver\nfrom sklearn import linear_model\n# declare a linear regression model \nlr = linear_model.LinearRegression(fit_intercept=True)\n# define response variable: energy usage\ny = np.asarray(dfScaled[response]) \n# define features\nX = dfScaled[features]\n# fit regression model to the data\nregr = lr.fit(X,y)\ncoefs = regr.coef_[0]\n# collect regression coefficients\ndataRegQ = []\ndataRegQ.append(('Intercept', regr.intercept_[0]))\nfor i in range(len(features)):\n    dataRegQ.append((features[i],coefs[i]))\n# compute energy predictions using our fitted model     \nyh = regr.predict(X)\n# import package to compute the R-squared quality metric\nfrom sklearn.metrics import r2_score\n# print results\nprint 'R-Squared: ', r2_score(y,yh)\npd.DataFrame(dataRegQ,columns=['feature_name','coefficient']) #.head()"
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1
}